{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk   \n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import json, bson\n",
    "import pathlib\n",
    "import requests\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import warp\n",
    "\n",
    "agent = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0',\n",
    "          'Accept-Language': 'pt-BR'}\n",
    "\n",
    "stores_path = r\"../../../Parte2/data/pages/*\"\n",
    "files_path  = stores_path + r\"/*\" \n",
    "stores      = [pathlib.Path(store).stem for store in glob.glob(stores_path)]\n",
    "file_list   = sorted(glob.glob(files_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files and Extract Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(value):\n",
    "    s = re.sub(\"[^0-9,]\", \"\", value);\n",
    "    s = s.replace(',','.')\n",
    "    if len(s):\n",
    "        return float(s)\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def to_float_en(value):\n",
    "    value = value.replace('\\xa0', ' ').replace('Â', '').strip()\n",
    "    s = re.sub(\"[^0-9.,]\", \"\", value);\n",
    "    s = s.replace(',','.')\n",
    "    find_groups = re.findall('\\d.\\d', s)\n",
    "    if len(find_groups) > 1:\n",
    "        return float(find_groups[0])\n",
    "    if len(s):\n",
    "        return float(s)\n",
    "    else:\n",
    "        return -1\n",
    "        \n",
    "def check_string(text):\n",
    "    if len(text) <= 2:\n",
    "        return False\n",
    "    if text[:3] == 'var':\n",
    "        return False\n",
    "    if text[0] == u'\\xa0':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100fdd651ae7404194b8d3c4349c3bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=434), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dictionary  = []\n",
    "pages_dict  = []\n",
    "\n",
    "\n",
    "for page in tqdm(file_list):\n",
    "    with open(page, \"r\", encoding='utf-8') as f:\n",
    "        doc= f.read()\n",
    "\n",
    "    s = BeautifulSoup(doc, \"html.parser\")\n",
    "    for script in s([\"script\", \"style\"]):\n",
    "        script.decompose()    # rip it out\n",
    "    \n",
    "    all_text = s.body.find_all(text=True)\n",
    "\n",
    "    html_page = [x for x in all_text if check_string(x)]\n",
    "    html_page = ' '.join(html_page)\n",
    "\n",
    "    clean_text = re.sub(r'[^\\w\\s$]','', html_page.lower())\n",
    "    clean_text = re.sub(r'[\\n\\t]','', clean_text)\n",
    "    #to lower case\n",
    "\n",
    "    clean_text = [text for text in clean_text.split(' ') if len(text) > 2]\n",
    "    \n",
    "    word_dict = defaultdict(lambda: 0)\n",
    "    words, counts = np.unique(clean_text, return_counts=True)\n",
    "    for word, count in zip(words, counts):\n",
    "        word_dict[word] = count\n",
    "\n",
    "    pages_dict.append(word_dict)\n",
    "\n",
    "    dictionary += list(words)\n",
    "dictionary = np.unique(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca257c3522f74478809e8dc0a3570eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20453), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inverted_index = {}\n",
    "for key in tqdm(dictionary):\n",
    "    \n",
    "    inverted_index[key] = []\n",
    "    for page_idx, page_dict in enumerate(pages_dict):\n",
    "        count = page_dict[key]\n",
    "        if count > 0 :\n",
    "            inverted_index[key].append((int(page_idx), int(count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../../data/index_data/'\n",
    "with open(save_path + 'inverted_index.json', 'w') as f:\n",
    "    json.dump(inverted_index, f)\n",
    "\n",
    "with open(save_path + 'inverted_index.bson', 'wb') as fp:\n",
    "    fp.write(bson.dumps(inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = [os.path.abspath(store_path) for store_path in stores_path]\n",
    "with open(save_path + 'store_path.json', 'w') as f:\n",
    "    json.dump(full_path, f)\n",
    "    \n",
    "#with open('store_path.bson', 'wb') as fp:\n",
    "#    fp.write(bson.dumps(dict(full_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path + 'inverted_index.bson', 'rb') as fp:\n",
    "    inverted_index_bson = bson.loads(fp.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path +'inverted_index.json') as json_file:\n",
    "    inverted_index_json = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inverted Index With Low Small Data Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_c = {}\n",
    "for key in tqdm(dictionary):\n",
    "    idx_count = 0\n",
    "    inverted_index_c[key] = []\n",
    "    for page_idx, page_dict in enumerate(pages_dict):\n",
    "        count = page_dict[key]\n",
    "        if count > 0 :\n",
    "            inverted_index_c[key].append((int(idx_count), int(count)))\n",
    "            idx_count = 0\n",
    "        idx_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../../data/index_data/'\n",
    "with open(save_path + 'inverted_index_sc.json', 'w') as f:\n",
    "    json.dump(inverted_index_c, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path + 'inverted_index_sc.bson', 'wb') as fp:\n",
    "    fp.write(bson.dumps(inverted_index_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low case only, remenber toremove non-ascii\n",
    "\n",
    "most_frequents = warp.most_frequents\n",
    "ranges = {\n",
    "    'price':[500, 1000, 2000, 5000, 999999],\n",
    "    'ram':[2, 4, 8, 16],\n",
    "    'hd':[4, 8, 16, 64, 1024],\n",
    "    'screen':[3, 5, 6, 7, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5250c1e4f652441db5a10d0cc20882d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=434), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_list = []\n",
    "for idx, url in enumerate(tqdm(file_list)):\n",
    "    for store in stores:\n",
    "        if store in url:\n",
    "            function = getattr(warp,f'get_fields_{store}')\n",
    "            dict_url = function(url)\n",
    "            dict_url['store'] = store\n",
    "            dict_url['idx']   = idx\n",
    "            dict_url['url']   = url\n",
    "            dict_list.append(dict_url)\n",
    "            #print(dict_url)\n",
    "            #print(idx, url, dict_url['Preço'])\n",
    "            \n",
    "unique_models = []\n",
    "att = 'model'\n",
    "for page_idx, page_dict in enumerate(dict_list):\n",
    "    att_model = page_dict[att]\n",
    "    att_model = att_model.replace('\\xa0', ' ').replace('Â', '').strip()\n",
    "    if len(att_model) < 1: continue\n",
    "    unique_models.append(att_model)\n",
    "unique_models = np.unique(unique_models)\n",
    "\n",
    "unique_screens = []\n",
    "att = 'screen'\n",
    "for page_idx, page_dict in enumerate(dict_list):\n",
    "    att_screen = page_dict[att]\n",
    "    att_screen = to_float_en(att_screen)\n",
    "\n",
    "    if att_screen > 100: continue\n",
    "    if att_screen < 1: continue\n",
    "    unique_screens.append(att_screen)\n",
    "unique_screens = np.unique(unique_screens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inverted Index for Most Frequent Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_att = {}\n",
    "last_value = 0\n",
    "for enum_value in ranges.keys():\n",
    "    for act_idx, target in enumerate((ranges[enum_value])):\n",
    "        if act_idx == 0:\n",
    "            last_value = 0\n",
    "        else:\n",
    "            last_value = ranges[enum_value][act_idx - 1]\n",
    "        index = f'{enum_value}.{target}'\n",
    "        inverted_index_att[index] = []\n",
    "        for page_idx, page_dict in enumerate(dict_list):\n",
    "            if enum_value == 'screen':\n",
    "                price = to_float_en(page_dict[enum_value])\n",
    "            else:\n",
    "                price = to_float(page_dict[enum_value])\n",
    "\n",
    "            if last_value < price < target :\n",
    "                inverted_index_att[index].append((int(page_idx), (price)))\n",
    "\n",
    "for model in unique_models:\n",
    "    index = f'model.{model}'\n",
    "    inverted_index_att[index] = []\n",
    "    for page_idx, page_dict in enumerate(dict_list):\n",
    "        att_model = page_dict['model']\n",
    "        att_model = att_model.replace('\\xa0', ' ').replace('Â', '').strip()\n",
    "\n",
    "        if model == att_model :\n",
    "            inverted_index_att[index].append(int(page_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Inverted Index of Att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../../data/index_data/'\n",
    "with open(save_path + 'inverted_index_att.json', 'w') as f:\n",
    "    json.dump(inverted_index_att, f)\n",
    "\n",
    "with open(save_path + 'inverted_index_att.bson', 'wb') as fp:\n",
    "    fp.write(bson.dumps(inverted_index_att))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
