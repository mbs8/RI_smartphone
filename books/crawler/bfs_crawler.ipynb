{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.robotparser as urobot\n",
    "\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/605.1.15'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "#function to parse the robots.txt of a url\n",
    "def getRobot(url):\n",
    "    try:\n",
    "        robotLink = url + \"/robots.txt\"\n",
    "        header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/605.1.15'}\n",
    "        rp = urobot.RobotFileParser()\n",
    "        rp.set_url(robotLink)\n",
    "        rp.read()\n",
    "        \n",
    "        return rp         \n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition of the bfs_crawler\n",
    "class bfs_crawler:\n",
    "    def __init__(self, url):\n",
    "        self.url = url                \n",
    "        self.robotParser = getRobot(url)     #robot parser to check if a link is valid or not\n",
    "        self.links_list = []                 #list of allowed links\n",
    "        \n",
    "#     def __init__(self):\n",
    "#         self.url = None\n",
    "#         self.robotParser = None\n",
    "#         self.links_list = []\n",
    "    \n",
    "    def get_links(self):\n",
    "        actual_link = self.url\n",
    "        link_count = 0\n",
    "        \n",
    "        while(len(self.links_list) < 200):\n",
    "            try:\n",
    "                if(\"http\" not in actual_link):\n",
    "                    actual_link = \"http://\" + actual_link\n",
    "                req = requests.get(actual_link, headers=header)\n",
    "                \n",
    "                if(req.status_code == 200):    \n",
    "                    soup = BeautifulSoup(req.text)\n",
    "                    pageLinks = soup.findAll(\"a\", href=True)\n",
    "                    for a in pageLinks:\n",
    "                        link = a[\"href\"]\n",
    "                        if(self.robotParser.can_fetch(\"*\", link)):\n",
    "                            self.links_list.append(link)\n",
    "                    \n",
    "            except Exception:\n",
    "                print(\"Link invÃ¡lido!\")\n",
    "            finally: \n",
    "                if(link_count >= len(self.links_list)):\n",
    "                    print(\"END\")\n",
    "                    return\n",
    "                actual_link = self.links_list[link_count]\n",
    "                link_count += 1\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
