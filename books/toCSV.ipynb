{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk   \n",
    "import unicodedata\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from utils import *\n",
    "import pathlib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import requests\n",
    "\n",
    "store_list    = [\"ricardoeletro\", \"magazineluiza\"]\n",
    "\n",
    "dataset_pages = f\"data/heuristic_pages/*/*\"\n",
    "dataset_links = f\"data/heusristic_links/heusristic_links/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_data = [(url, pathlib.Path(url).stem.split('_')[0] ) for url in glob.glob(dataset_links) if len(url.split('_'))>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "\n",
    "dataset_idx = 0\n",
    "data_raw_s = pd.read_csv(dataset_data[dataset_idx][0])\n",
    "data_raw_s = data_raw_s.dropna().drop_duplicates()\n",
    "data_raw_s['store'] = np.array([dataset_data[dataset_idx][1]] * data_raw_s.shape[0])\n",
    "\n",
    "for dataset_idx in range(1, len(dataset_data)):\n",
    "    data_raw = pd.read_csv(dataset_data[dataset_idx][0])\n",
    "    data_raw = data_raw.dropna().drop_duplicates()\n",
    "    data_raw['store'] = np.array([dataset_data[dataset_idx][1]] * data_raw.shape[0])\n",
    "    data_raw_s = pd.merge(data_raw_s, data_raw, how='outer')\n",
    "    #datasets.append(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pages = [f'data/heuristic_pages/{store}/page_{p_id}.html' for (store, p_id) in zip(data_raw_s['store'], data_raw_s['id'])]\n",
    "y = data_raw_s['label'].values\n",
    "u_class = data_raw_s['store']\n",
    "l = data_raw_s['links'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d7489e04b84b7795168f7e61a4e418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2709), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Invalid Links 4\n"
     ]
    }
   ],
   "source": [
    "X, idx_arr = pages_to_matrix(x_pages)\n",
    "y = y[idx_arr]\n",
    "u_class = u_class[idx_arr]\n",
    "l = l[idx_arr]\n",
    "X.shape, y.shape, u_class.shape, l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['store'] = u_class\n",
    "df['link'] = l\n",
    "df['text'] = X\n",
    "df['label'] = y\n",
    "df.to_csv(\"text_store_label.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clss = [GaussianNB(), MultinomialNB(), KNeighborsClassifier(), SVC(gamma='auto'), \n",
    "        RandomForestClassifier(n_estimators=200), GradientBoostingClassifier(n_estimators=200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB 0.767097966728281\n",
      "MultinomialNB 0.9537892791127541\n",
      "KNeighborsClassifier 0.9759704251386322\n",
      "SVC 0.9593345656192237\n",
      "RandomForestClassifier 0.9796672828096118\n",
      "GradientBoostingClassifier 0.9815157116451017\n"
     ]
    }
   ],
   "source": [
    "CV = CountVectorizer()\n",
    "    \n",
    "for cls in clss:\n",
    "\n",
    "    X_vec = CV.fit_transform(X_train).toarray()\n",
    "    X_vec_test = CV.transform(X_test).toarray()\n",
    "    \n",
    "    cls.fit(X_vec, y_train)  \n",
    "    predicted = cls.predict(X_vec_test)\n",
    "    print(cls.__class__.__name__, np.mean(predicted == y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseTransformer(MinMaxScaler):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB 0.7689463955637708\n",
      "MultinomialNB 0.9630314232902033\n",
      "KNeighborsClassifier 0.9685767097966729\n",
      "SVC 0.7652495378927912\n"
     ]
    }
   ],
   "source": [
    "CV = CountVectorizer()\n",
    "    \n",
    "for cls in clss:\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('tranf', DenseTransformer()),\n",
    "        ('clf', cls),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    text_clf.fit(X_train, y_train)  \n",
    "    predicted = text_clf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(cls.__class__.__name__, np.mean(predicted == y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9630314232902033"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)  \n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
