{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils.ipynb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.robotparser as urobot\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import validators\n",
    "import os\n",
    "import threading\n",
    "import logging\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/605.1.15'}\n",
    "\n",
    "urls = [\n",
    "        \"https://www.magazineluiza.com.br\",\n",
    "        \"https://www.colombo.com.br\",\n",
    "        #\"https://www.amazon.com.br\",\n",
    "        \"https://www.taqi.com.br\",\n",
    "        #\"https://www.kabum.com.br\",\n",
    "        \"https://www.ibyte.com.br\",\n",
    "        #\"https://www.cissamagazine.com.br\",\n",
    "        #\"https://www.ricardoeletro.com.br\",\n",
    "        \"https://www.havan.com.br\",\n",
    "        #\"https://www.avenida.com.br\"\n",
    "        ]\n",
    "\n",
    "#urls = [\"https://www.avenida.com.br\"]\n",
    "\n",
    "#to control concurrent executions\n",
    "LOCK = threading.Lock()\n",
    "\n",
    "#configures logger to track progress of the threads\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matheus/anaconda3/lib/python3.7/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/Matheus/anaconda3/lib/python3.7/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.21.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/Matheus/anaconda3/lib/python3.7/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.21.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/Matheus/anaconda3/lib/python3.7/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator Pipeline from version 0.21.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "class DenseTransformer(MinMaxScaler):\n",
    " \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "#carrega o modelo de classificador de links\n",
    "filename = \"link_clf3.sav\"\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition of the classifier_crawler\n",
    "class classifier_crawler:\n",
    "    def __init__(self, url):\n",
    "        self.url = url                \n",
    "        self.robotParser = getRobot(url)           #robot parser to check if a link is valid or not\n",
    "        self.links_list = []                       #list of allowed links\n",
    "        self.invalid_links = []                    #list of invalid links\n",
    "        self.file_name  = set_file_name(url)       #name of the files generate by the crawler \n",
    "    \n",
    "    #saves the relevant links and pages from the site\n",
    "    def get_links(self):\n",
    "        actual_link = self.url\n",
    "        number_of_links = 500\n",
    "        next_links = []\n",
    "        total_links = 0\n",
    "        visited_links = [self.url]\n",
    "        \n",
    "        with LOCK:\n",
    "            print(\"Starts crawler on: {}\".format(self.url))\n",
    "            pbar = tqdm(total=number_of_links)\n",
    "        \n",
    "        while (len(self.links_list) < number_of_links):\n",
    "            try:\n",
    "                req = requests.get(actual_link, headers=header)\n",
    "                if(req.status_code == 200):    \n",
    "                    soup = BeautifulSoup(req.text)\n",
    "                    pageLinks = soup.findAll(\"a\", href=True)\n",
    "                    \n",
    "                    for a in pageLinks:\n",
    "                        #formata o link na tentativa de obter um link válido\n",
    "                        link = format_link(self.url, a[\"href\"])\n",
    "                        print(link)\n",
    "                        #verifica se o link é valido\n",
    "                        if(not validators.url(link)):\n",
    "                            if(link not in self.invalid_links):\n",
    "                                self.invalid_links.append(link)\n",
    "                                \n",
    "                        elif(link not in visited_links):\n",
    "                            next_links.append(link)\n",
    "                            total_links += 1\n",
    "                            #adiciona na lista de links do crawler caso seja um link não visitado, válido e que o robots.txt permita\n",
    "                            if((self.robotParser.can_fetch(\"*\", link)) and (link != self.url) and classifier_check([link])):\n",
    "                                self.links_list.append(link)\n",
    "                            \n",
    "                                pbar.update(1)\n",
    "                                if(len(self.links_list) >= number_of_links):\n",
    "                                    break  \n",
    "                        #print(link)\n",
    "                        \n",
    "                        visited_links.append(link)\n",
    "                \n",
    "            except Exception:\n",
    "                if(link not in self.invalid_links):\n",
    "                    print(\"except\")\n",
    "                    self.invalid_links.append(link)\n",
    "            finally: \n",
    "                if(len(next_links) < 1 or len(self.links_list) >= number_of_links):\n",
    "                    logging.info(\"****END: {}****\".format(self.url))\n",
    "                    with LOCK:\n",
    "                        #saves the results to a file stats.csv; columns: site,valid_links,invalid_links,total_links\n",
    "                        content = \"{},{},{},{}\\n\".format(self.url.split(\".\")[1],len(self.links_list),len(self.invalid_links),total_links)\n",
    "                        save_file(content, \"./\", \"stats.csv\", mode=\"a\")\n",
    "                        pbar.close()\n",
    "                    return\n",
    "                actual_link = next_links.pop(0)\n",
    "                #print(\"Link Atual: {}\".format(actual_link))\n",
    "                time.sleep(2)\n",
    "    \n",
    "    #saves the links as csv \n",
    "    def save_as_csv(self):\n",
    "        folders = [\"links\", \"invalid_links\"]\n",
    "        \n",
    "        for folder in folders:\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "                \n",
    "        #saves valid links\n",
    "        ID = np.arange(len(self.links_list))\n",
    "        dictionary = {'id' : ID, 'links' : self.links_list}\n",
    "        df = pd.DataFrame(dictionary)\n",
    "        df.to_csv(('links/' + self.file_name + '.csv'),header=True, index=False, encoding='utf-8')\n",
    "        \n",
    "        #saves invalid links\n",
    "        ID = np.arange(len(self.invalid_links))\n",
    "        dictionary = {'id' : ID, 'links' : self.invalid_links}\n",
    "        df = pd.DataFrame(dictionary)\n",
    "        df.to_csv(('invalid_links/' + self.file_name + '.csv'),header=True, index=False, encoding='utf-8')\n",
    "    \n",
    "    #save the robots from a url\n",
    "    def save_robot(self):\n",
    "        robot_url = self.url + \"/robots.txt\"\n",
    "        req = requests.get(robot_url,headers=header)\n",
    "        content = req.text\n",
    "        save_file(content, \"robots\", (self.file_name + \".txt\"))\n",
    "    \n",
    "    def classifier_check(self, link):  \n",
    "        result = loaded_model.predict(link).astype(bool)\n",
    "        return result[0]\n",
    "    \n",
    "    def save_page(self, identifier, link):\n",
    "        PAGE_NAME = \"page_{}.html\"\n",
    "        SITE_NAME = self.url.split(\".\")[1]\n",
    "        PAGES_DIRECTORY = \"pages/\" + SITE_NAME\n",
    "        \n",
    "        time.sleep(random.randint(1,5))\n",
    "        try: \n",
    "            req = requests.get(link, headers=header)\n",
    "            if(req.status_code == 200):\n",
    "                html = req.text\n",
    "                save_file(html, PAGES_DIRECTORY, PAGE_NAME.format(identifier))\n",
    "                return True\n",
    "            else: \n",
    "                logging.info(\"SAVE_PAGE:status_code: {} - {}\".format(req.status_code, link))\n",
    "                return False\n",
    "        except:\n",
    "            logging.info(\"EXCEPT_SAVE_PAGE: {} - {}\".format(req.status_code, link))\n",
    "            return False\n",
    "        \n",
    "    #getters and setters methods\n",
    "    def get_url(self):\n",
    "        return self.url\n",
    "    def set_url(self, url):\n",
    "        self.url = url\n",
    "    def get_links_list(self):\n",
    "        return self.links_list\n",
    "    def set_links_list(self, links):\n",
    "        self.links_list = links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the links from each site\n",
    "def save_links(crawler):\n",
    "    crawler = classifier_crawler(url)\n",
    "    crawler.save_robot()\n",
    "    crawler.get_links()\n",
    "    crawler.save_as_csv()\n",
    "    \n",
    "def classifier_check(link):  \n",
    "    result = loaded_model.predict(link).astype(bool)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel saves the links and pages from all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts crawler on: https://www.magazineluiza.com.br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8505154e9f24c389ca713993438f46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts crawler on: https://www.colombo.com.br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccb5a02cedf46289b05b2c51e8d11d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts crawler on: https://www.taqi.com.br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5e9c71374f48cdbac052c7d59aa194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts crawler on: https://www.havan.com.br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788f03d89b284d8bbdb7283a52801abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts crawler on: https://www.ibyte.com.br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbb17547a7649e6aff6c0758232828f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threads = []\n",
    "for url in urls:\n",
    "    crawler = classifier_crawler(url)\n",
    "    thread = threading.Thread(target=save_links, args=(crawler,))\n",
    "    threads.append(thread)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves the links and pages from one site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = urls[1]\n",
    "crawler = classifier_crawler(url)\n",
    "save_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
